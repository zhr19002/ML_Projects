{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476ba84b-a1d4-4fe5-b2a4-7ef51c0f2ef5",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa825f1",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52eec05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load raw datasets\n",
    "df_wave = pd.read_csv('WLIS_wave_raw.csv')\n",
    "df_wind = pd.read_csv('WLIS_wind_raw.csv')\n",
    "\n",
    "# Set the timestamp column as the index\n",
    "df_wave['TmStamp'] = pd.to_datetime(df_wave['TmStamp'], format='mixed')\n",
    "df_wind['TmStamp'] = pd.to_datetime(df_wind['TmStamp'], format='mixed')\n",
    "df_wave.set_index('TmStamp', inplace=True)\n",
    "df_wind.set_index('TmStamp', inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "df_wave = df_wave.rename(columns={'Hsig_m':'H'})\n",
    "df_wind = df_wind.rename(columns={'windSpd_Kts':'WSPD', 'windDir_M':'WDIR'})\n",
    "\n",
    "# In total 118,872 hourly timestamps\n",
    "date_ranges = [\n",
    "    ('2004-11-01 00:00:00', '2015-01-05 23:00:00'),  # 89232 hourly timestamps\n",
    "    ('2016-12-12 00:00:00', '2018-01-11 23:00:00'),  # 9504 hourly timestamps\n",
    "    ('2018-04-27 00:00:00', '2018-10-22 23:00:00'),  # 4296 hourly timestamps\n",
    "    ('2019-05-28 00:00:00', '2019-12-31 23:00:00'),  # 5232 hourly timestamps\n",
    "    ('2022-12-09 00:00:00', '2023-06-02 23:00:00'),  # 4224 hourly timestamps\n",
    "    ('2024-03-28 00:00:00', '2024-12-18 23:00:00')]  # 6384 hourly timestamps\n",
    "\n",
    "# Filter date ranges\n",
    "ndf_wave = pd.DataFrame()\n",
    "ndf_wind = pd.DataFrame()\n",
    "date_ranges[0] = ('2006-02-23 00:00:00', '2015-01-05 23:00:00')\n",
    "for start, end in [(pd.to_datetime(start), pd.to_datetime(end[:-5] + '59:59')) for start, end in date_ranges]:\n",
    "    ndf_wave = pd.concat([ndf_wave, df_wave[(df_wave.index >= start) & (df_wave.index <= end)]])\n",
    "    ndf_wind = pd.concat([ndf_wind, df_wind[(df_wind.index >= start) & (df_wind.index <= end)]])\n",
    "\n",
    "# Remove outliers\n",
    "ndf_wind.loc[ndf_wind['WSPD'] > 50, 'WSPD'] = np.nan\n",
    "\n",
    "# Resample by hour\n",
    "ndf_wave = ndf_wave.resample('h').max()\n",
    "ndf_wind = ndf_wind.resample('h').max()\n",
    "\n",
    "# Convert units\n",
    "ndf_wave['H'] = 3.28084 * ndf_wave['H']         # m to ft\n",
    "ndf_wind['WSPD'] = 0.514444 * ndf_wind['WSPD']  # kts to m/s\n",
    "\n",
    "# Fill in the missing timestamps\n",
    "timestamps = []\n",
    "date_ranges[0] = ('2004-11-01 00:00:00', '2015-01-05 23:00:00')\n",
    "for start, end in [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in date_ranges]:\n",
    "    timestamps.extend(pd.date_range(start, end, freq='h'))\n",
    "\n",
    "# Merge datasets\n",
    "df_merge = pd.merge(ndf_wave, ndf_wind, how='outer', left_index=True, right_index=True)\n",
    "df_merge = pd.merge(pd.DataFrame(timestamps, columns=['TmStamp']), df_merge, on='TmStamp', how='left')\n",
    "df_merge = df_merge.sort_values('TmStamp').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca46bc",
   "metadata": {},
   "source": [
    "#### Data Imputation Using 2004-2013 Buoy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701ba80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2004-2013 buoy dataset\n",
    "df_buoy = pd.read_csv('Data_Buoy_2004_2013.csv')\n",
    "df_buoy = df_buoy.rename(columns={'TimeStamp_1':'TmStamp', 'WSPD':'WSPD1', 'WDIR':'WDIR1'})\n",
    "df_buoy['TmStamp'] = pd.to_datetime(df_buoy['TmStamp'], format='mixed')\n",
    "\n",
    "# First imputation\n",
    "df_new = pd.merge(df_merge, df_buoy[['TmStamp','SWHft','WSPD1','WDIR1']], on='TmStamp', how='left')\n",
    "df_new['H'] = df_new['H'].fillna(df_new['SWHft'])\n",
    "df_new['WSPD'] = df_new['WSPD'].fillna(df_new['WSPD1'])\n",
    "df_new['WDIR'] = df_new['WDIR'].fillna(df_new['WDIR1'])\n",
    "df_new = df_new[['TmStamp','H','WSPD','WDIR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb1ab4",
   "metadata": {},
   "source": [
    "#### Wind Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c515d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count consecutive NaN lengths\n",
    "def get_nan_lengths(series):\n",
    "    nans = series.isna()\n",
    "    group = (nans != nans.shift()).cumsum()\n",
    "    nan_lengths = nans.groupby(group).transform('sum')\n",
    "    return np.where(nans, nan_lengths, 0).astype(int)\n",
    "\n",
    "# Case 1: Interpolate when nan_lengths â‰¤ 5\n",
    "WDIR = df_new['WDIR'].interpolate(method='linear')\n",
    "WSPD = df_new['WSPD'].interpolate(method='linear')\n",
    "n_random = np.random.normal(loc=0, scale=1, size=len(df_new))\n",
    "sd_WSPD = np.std(df_new['WSPD'].dropna())\n",
    "WSPD += n_random * sd_WSPD\n",
    "\n",
    "WDIR_nans = get_nan_lengths(df_new['WDIR'])\n",
    "WSPD_nans = get_nan_lengths(df_new['WSPD'])\n",
    "\n",
    "WDIR_mask = (WDIR_nans > 0) & (WDIR_nans <= 5)\n",
    "WSPD_mask = (WSPD_nans > 0) & (WSPD_nans <= 5)\n",
    "\n",
    "df_new.loc[WDIR_mask, 'WDIR'] = WDIR[WDIR_mask]\n",
    "df_new.loc[WSPD_mask, 'WSPD'] = WSPD[WSPD_mask]\n",
    "\n",
    "# Case 2: Sample from known ('WSPD', 'WDIR') when nan_lengths > 5\n",
    "df_new['season'] = np.where(df_new['TmStamp'].dt.month.isin([11, 12, 1, 2, 3]), 'windy', 'calm')\n",
    "for season in ['windy','calm']:\n",
    "    df_season = df_new[df_new['season'] == season]\n",
    "    mask = (WSPD_nans > 5) & (df_new['season'] == season)\n",
    "    sampled = df_season[['WSPD','WDIR']].dropna().sample(n=mask.sum(), replace=True)\n",
    "    df_new.loc[mask, ['WSPD','WDIR']] = sampled.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc8df8",
   "metadata": {},
   "source": [
    "#### Wave Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954c5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged WSPD quartile bins\n",
    "df_new['rWSPD'] = df_new['WSPD'].shift(1)\n",
    "quartiles = df_new['rWSPD'].quantile([0.25, 0.5, 0.75])\n",
    "df_new['bin'] = pd.cut(\n",
    "    df_new['rWSPD'],\n",
    "    bins = [-np.inf, quartiles[0.25], quartiles[0.5], quartiles[0.75], np.inf],\n",
    "    labels = ['Q1','Q2','Q3','Q4'])\n",
    "\n",
    "# Impute NaN with the median wave height for each bin categorized by season\n",
    "damp = {'windy':0.03, 'calm':0.01}\n",
    "for season in ['windy','calm']:\n",
    "    df_season = df_new[df_new['season'] == season]\n",
    "    for label in ['Q1','Q2','Q3','Q4']:\n",
    "        mask = (df_new['H'].isna()) & (df_new['bin'] == label) & (df_new['season'] == season)\n",
    "        obs_median = df_season[df_season['bin'] == label]['H'].median()\n",
    "        n_random = np.random.normal(loc=0, scale=1, size=mask.sum())\n",
    "        df_new.loc[mask, 'H'] = obs_median + damp[season] * n_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e85981",
   "metadata": {},
   "source": [
    "#### Fetch Calculation & Processed Dataset Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea4e268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TmStamp</th>\n",
       "      <th>H</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>WDIR</th>\n",
       "      <th>Fetch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-11-01 00:00:00</td>\n",
       "      <td>3.249133</td>\n",
       "      <td>13.4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>6437.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-11-01 01:00:00</td>\n",
       "      <td>3.249133</td>\n",
       "      <td>8.2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>98169.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-11-01 02:00:00</td>\n",
       "      <td>1.960359</td>\n",
       "      <td>7.7</td>\n",
       "      <td>340.0</td>\n",
       "      <td>8046.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-11-01 03:00:00</td>\n",
       "      <td>2.001249</td>\n",
       "      <td>10.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8046.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-11-01 04:00:00</td>\n",
       "      <td>3.254957</td>\n",
       "      <td>11.8</td>\n",
       "      <td>50.0</td>\n",
       "      <td>27358.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TmStamp         H  WSPD   WDIR     Fetch\n",
       "0 2004-11-01 00:00:00  3.249133  13.4  120.0   6437.36\n",
       "1 2004-11-01 01:00:00  3.249133   8.2   80.0  98169.74\n",
       "2 2004-11-01 02:00:00  1.960359   7.7  340.0   8046.70\n",
       "3 2004-11-01 03:00:00  2.001249  10.3   10.0   8046.70\n",
       "4 2004-11-01 04:00:00  3.254957  11.8   50.0  27358.78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_data = [\n",
    "    (0, 3), (10, 5), (20, 5), (30, 10), (40, 12), (50, 17), (60, 40), (70, 75),\n",
    "    (80, 61), (90, 22), (100, 17), (110, 11), (120, 4), (130, 4), (140, 3), (150, 3),\n",
    "    (160, 3), (170, 3), (180, 3), (190, 3), (200, 3), (210, 4), (220, 5), (230, 10),\n",
    "    (240, 14), (250, 11), (260, 8), (270, 7), (280, 6), (290, 5), (300, 5), (310, 5),\n",
    "    (320, 5), (330, 5), (340, 5), (350, 4), (360, 3)]\n",
    "\n",
    "fetch_df = pd.DataFrame(fetch_data, columns=['Deg','Dist'])\n",
    "df_new['Fetch'] = np.interp(df_new['WDIR'], fetch_df['Deg'], fetch_df['Dist'])\n",
    "df_new['Fetch'] = 1609.34 * df_new['Fetch']\n",
    "\n",
    "# Save the processed dataset\n",
    "df_new = df_new[['TmStamp','H','WSPD','WDIR','Fetch']]\n",
    "df_new = df_new.sort_values('TmStamp').reset_index(drop=True)\n",
    "df_new.to_csv('WLIS_data.csv', index=False)\n",
    "\n",
    "df_new.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
